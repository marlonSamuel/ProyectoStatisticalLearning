{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train.csv')\n",
    "Y_train = pd.read_csv('Y_train.csv')\n",
    "X_valid = pd.read_csv('X_valid.csv')\n",
    "Y_valid = pd.read_csv('Y_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self._classes = np.unique(y)\n",
    "        n_classes = len(self._classes)\n",
    "\n",
    "        # calculate mean, var, and prior for each class\n",
    "        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._priors =  np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            X_c = X[y==c]\n",
    "            self._mean[idx, :] = X_c.mean(axis=0)\n",
    "            self._var[idx, :] = X_c.var(axis=0)\n",
    "            self._priors[idx] = X_c.shape[0] / float(n_samples)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        posteriors = []\n",
    "\n",
    "        # calculate posterior probability for each class\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            prior = np.log(self._priors[idx])\n",
    "            posterior = np.sum(np.log(self._pdf(idx, x)))\n",
    "            posterior = prior + posterior\n",
    "            posteriors.append(posterior)\n",
    "            \n",
    "        # return class with highest posterior probability\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "            \n",
    "\n",
    "    def _pdf(self, class_idx, x):\n",
    "        mean = self._mean[class_idx]\n",
    "        var = self._var[class_idx]\n",
    "        numerator = np.exp(- (x-mean)**2 / (2 * var))\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        return numerator / denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes()\n",
    "nb.fit(X_train, np.array(Y_train))\n",
    "\n",
    "predictions = nb.predict(np.array(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(0.6867816091954023, 0.4644701837487957),\n",
       "  (0.9741379310344828, 0.15895227365661044),\n",
       "  (0.6752873563218391, 0.46894163522342186),\n",
       "  (0.14655172413793102, 0.3541677018104052),\n",
       "  (0.7758620689655172, 0.4176137832812562),\n",
       "  (0.07758620689655173, 0.2679045187444013)],\n",
       " 1: [(0.49321266968325794, 0.5010889006795146),\n",
       "  (0.8959276018099548, 0.30604764068857054),\n",
       "  (0.36199095022624433, 0.48166719924323365),\n",
       "  (0.6561085972850679, 0.4760837082320005),\n",
       "  (0.665158371040724, 0.47300643594032665),\n",
       "  (0.16289592760180996, 0.37010899258519975)]}"
      ]
     },
     "execution_count": 812,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = Y_train[\"passenger_survived\"].unique()\n",
    "means = []\n",
    "variances = []\n",
    "priors = []\n",
    "for c in classes:\n",
    "    f = X_train[np.array(Y_train)==c]\n",
    "    mean_s = f.mean()\n",
    "    var = f.std()\n",
    "    prior = f.shape[0]/len(X_train)\n",
    "    means.append(np.array(mean_s))\n",
    "    variances.append(np.array(var))\n",
    "    priors.append(np.array(prior))\n",
    "\n",
    "info = {\n",
    "   0:list(zip(means[1],variances[1])),\n",
    "   1:list(zip(means[0],variances[0]))\n",
    "}\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Gaussian Probability Density Function \n",
    "def calculateGaussianProbability(x, mean, stdev): \n",
    "    expo = np.exp(-((x - mean)**2 / (2 * (stdev**2)))) \n",
    "    return (1 / (np.sqrt(2 * np.pi) * stdev)) * expo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Class Probabilities \n",
    "def calculateClassProbabilities(info, test): \n",
    "    probabilities = {} \n",
    "    for classValue, classSummaries in info.items(): \n",
    "        probabilities[classValue] = 1\n",
    "        for i in range(len(classSummaries)): \n",
    "            mean, std_dev = classSummaries[i] \n",
    "            x = test[i] \n",
    "            probabilities[classValue] *= calculateGaussianProbability(x, mean, std_dev) \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(info, test): \n",
    "    probabilities = calculateClassProbabilities(info, test) \n",
    "    print(probabilities)\n",
    "    bestLabel, bestProb = None, -1\n",
    "    for classValue, probability in probabilities.items(): \n",
    "        if bestLabel is None or probability > bestProb: \n",
    "            bestProb = probability \n",
    "            bestLabel = classValue \n",
    "    return bestLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns predictions for a set of examples \n",
    "def getPredictions(info, test): \n",
    "    predictions = [] \n",
    "    for i in range(len(test)): \n",
    "        result = predict(info, test[i]) \n",
    "        predictions.append(result) \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.03482045318425356, 1: 0.0861930993346523}\n",
      "{0: 0.6243380624252367, 1: 0.07637604425946255}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 0.08276511415546238, 1: 0.08389438325802143}\n",
      "{0: 4.361636338704468e-05, 1: 0.013336268989321987}\n",
      "{0: 0.03482045318425356, 1: 0.0861930993346523}\n",
      "{0: 0.05400755315024579, 1: 0.03750646725405985}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 0.037296753608422425, 1: 0.15208153957556392}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 0.0016202219821762505, 1: 0.003694633746046651}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 0.2848680140361448, 1: 0.02013830589672188}\n",
      "{0: 4.730273232287483e-05, 1: 0.0034226343688205994}\n",
      "{0: 0.1198479993370759, 1: 0.02069009787281318}\n",
      "{0: 0.0016202219821762505, 1: 0.003694633746046651}\n",
      "{0: 0.0016202219821762505, 1: 0.003694633746046651}\n",
      "{0: 0.26266784617720174, 1: 0.0784687569535738}\n",
      "{0: 0.08276511415546238, 1: 0.08389438325802143}\n",
      "{0: 2.2842564943702947e-11, 1: 0.0010900329983690852}\n",
      "{0: 0.08276511415546238, 1: 0.08389438325802143}\n",
      "{0: 0.037296753608422425, 1: 0.15208153957556392}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 5.4294741230538795e-11, 1: 0.001060962499724169}\n",
      "{0: 9.678894127556679e-05, 1: 0.007356830190861754}\n",
      "{0: 0.2848680140361448, 1: 0.02013830589672188}\n",
      "{0: 0.5828854302460316, 1: 0.04328656842912058}\n",
      "{0: 0.26266784617720174, 1: 0.0784687569535738}\n",
      "{0: 0.26266784617720174, 1: 0.0784687569535738}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 0.000730126704672577, 1: 0.00669753523949869}\n",
      "{0: 9.088800032209567e-10, 1: 0.0005328202164622431}\n",
      "{0: 0.1198479993370759, 1: 0.02069009787281318}\n",
      "{0: 0.5828854302460316, 1: 0.04328656842912058}\n",
      "{0: 0.6243380624252367, 1: 0.07637604425946255}\n",
      "{0: 0.0032263072269115498, 1: 0.07468364379621817}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 1.990092798328164e-05, 1: 0.003516414957490521}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 0.5828854302460316, 1: 0.04328656842912058}\n",
      "{0: 2.2842564943702947e-11, 1: 0.0010900329983690852}\n",
      "{0: 0.5828854302460316, 1: 0.04328656842912058}\n",
      "{0: 0.2848680140361448, 1: 0.02013830589672188}\n",
      "{0: 0.017017466609604622, 1: 0.04009980609643414}\n",
      "{0: 0.12837114089481988, 1: 0.03650619321913054}\n",
      "{0: 0.0016202219821762505, 1: 0.003694633746046651}\n",
      "{0: 0.00033313641578553956, 1: 0.0017659596633323752}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 0.26266784617720174, 1: 0.0784687569535738}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 0.2848680140361448, 1: 0.02013830589672188}\n",
      "{0: 0.015691271330905324, 1: 0.1562485918430075}\n",
      "{0: 0.0032263072269115498, 1: 0.07468364379621817}\n",
      "{0: 0.037296753608422425, 1: 0.15208153957556392}\n",
      "{0: 0.6243380624252367, 1: 0.07637604425946255}\n",
      "{0: 0.6243380624252367, 1: 0.07637604425946255}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 0.6243380624252367, 1: 0.07637604425946255}\n",
      "{0: 0.26266784617720174, 1: 0.0784687569535738}\n",
      "{0: 0.007159488698116271, 1: 0.04119854555149582}\n",
      "{0: 0.017017466609604622, 1: 0.04009980609643414}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 0.015691271330905324, 1: 0.1562485918430075}\n",
      "{0: 0.2848680140361448, 1: 0.02013830589672188}\n",
      "{0: 0.26266784617720174, 1: 0.0784687569535738}\n",
      "{0: 0.2848680140361448, 1: 0.02013830589672188}\n",
      "{0: 0.015691271330905324, 1: 0.1562485918430075}\n",
      "{0: 4.361636338704468e-05, 1: 0.013336268989321987}\n",
      "{0: 0.037296753608422425, 1: 0.15208153957556392}\n",
      "{0: 0.0038511232637445498, 1: 0.0035961001736973626}\n",
      "{0: 0.017017466609604622, 1: 0.04009980609643414}\n",
      "{0: 3.088080406497598e-13, 1: 0.00019464734893162}\n",
      "{0: 0.2848680140361448, 1: 0.02013830589672188}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 0.015691271330905324, 1: 0.1562485918430075}\n",
      "{0: 0.015691271330905324, 1: 0.1562485918430075}\n",
      "{0: 9.678894127556679e-05, 1: 0.007356830190861754}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 0.08276511415546238, 1: 0.08389438325802143}\n",
      "{0: 0.26266784617720174, 1: 0.0784687569535738}\n",
      "{0: 0.6243380624252367, 1: 0.07637604425946255}\n",
      "{0: 5.4294741230538795e-11, 1: 0.001060962499724169}\n",
      "{0: 0.03482045318425356, 1: 0.0861930993346523}\n",
      "{0: 0.6243380624252367, 1: 0.07637604425946255}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 0.03482045318425356, 1: 0.0861930993346523}\n",
      "{0: 0.037296753608422425, 1: 0.15208153957556392}\n",
      "{0: 8.968029768885756e-06, 1: 0.006374464889704843}\n",
      "{0: 8.968029768885756e-06, 1: 0.006374464889704843}\n",
      "{0: 0.12837114089481988, 1: 0.03650619321913054}\n",
      "{0: 0.03482045318425356, 1: 0.0861930993346523}\n",
      "{0: 0.26266784617720174, 1: 0.0784687569535738}\n",
      "{0: 0.007159488698116271, 1: 0.04119854555149582}\n",
      "{0: 0.26266784617720174, 1: 0.0784687569535738}\n",
      "{0: 0.0032263072269115498, 1: 0.07468364379621817}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 0.017017466609604622, 1: 0.04009980609643414}\n",
      "{0: 0.0038511232637445498, 1: 0.0035961001736973626}\n",
      "{0: 0.6243380624252367, 1: 0.07637604425946255}\n",
      "{0: 0.6243380624252367, 1: 0.07637604425946255}\n",
      "{0: 0.5828854302460316, 1: 0.04328656842912058}\n",
      "{0: 0.26266784617720174, 1: 0.0784687569535738}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 0.2848680140361448, 1: 0.02013830589672188}\n",
      "{0: 0.6243380624252367, 1: 0.07637604425946255}\n",
      "{0: 0.2848680140361448, 1: 0.02013830589672188}\n",
      "{0: 0.6243380624252367, 1: 0.07637604425946255}\n",
      "{0: 0.6243380624252367, 1: 0.07637604425946255}\n",
      "{0: 0.015691271330905324, 1: 0.1562485918430075}\n",
      "{0: 0.5828854302460316, 1: 0.04328656842912058}\n",
      "{0: 0.03482045318425356, 1: 0.0861930993346523}\n",
      "{0: 0.03482045318425356, 1: 0.0861930993346523}\n",
      "{0: 0.000730126704672577, 1: 0.00669753523949869}\n",
      "{0: 0.0032263072269115498, 1: 0.07468364379621817}\n",
      "{0: 0.2848680140361448, 1: 0.02013830589672188}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 0.05400755315024579, 1: 0.03750646725405985}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 0.1198479993370759, 1: 0.02069009787281318}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 0.007668644762403078, 1: 0.07269187770379365}\n",
      "{0: 3.823786618938754e-10, 1: 0.0005474195537476577}\n",
      "{0: 0.08276511415546238, 1: 0.08389438325802143}\n",
      "{0: 0.2848680140361448, 1: 0.02013830589672188}\n",
      "{0: 0.5828854302460316, 1: 0.04328656842912058}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 0.6243380624252367, 1: 0.07637604425946255}\n",
      "{0: 0.6243380624252367, 1: 0.07637604425946255}\n",
      "{0: 0.12837114089481988, 1: 0.03650619321913054}\n",
      "{0: 0.6243380624252367, 1: 0.07637604425946255}\n",
      "{0: 1.1109570763618275e-10, 1: 0.0022805009557689167}\n",
      "{0: 0.08276511415546238, 1: 0.08389438325802143}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 1.3854667232098163, 1: 0.04213214270921613}\n",
      "{0: 0.037296753608422425, 1: 0.15208153957556392}\n",
      "{0: 0.015691271330905324, 1: 0.1562485918430075}\n"
     ]
    }
   ],
   "source": [
    "# prepare model \n",
    "test = np.array(X_valid)\n",
    "\n",
    "# test model \n",
    "predictions = getPredictions(info, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metricsaccuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7884615384615384"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.recall_score(Y_valid,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teorema de Bayes\n",
    "\n",
    "$P(y|X) = \\frac{P(X|y).P(y)}{P(X)}$\n",
    "\n",
    "* Para multiples caracteristicas del vector X\n",
    "\n",
    "$X = (X_1,X_2,X_3.....X_n)$\n",
    "\n",
    "$P(y|X) = \\frac{P(X_1|y).P(X_2|y).P(X_3|y)...P(X_n|y).P(y)}{P(X)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "passenger_survived\n",
       "0    0.611599\n",
       "1    0.388401\n",
       "dtype: float64"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_probs = data.groupby('passenger_survived').size().div(len(data))\n",
    "rating_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = X_train.columns.values\n",
    "s0 = []\n",
    "s1 = []\n",
    "p0 = []\n",
    "p1 = []\n",
    "\n",
    "for d in data[columns]:\n",
    "    a = data.groupby([d, 'passenger_survived']).size().div(len(data)).div(rating_probs, axis=0, level='passenger_survived')\n",
    "    #a = pd.crosstab(data[d],data.passenger_survived,normalize='columns')\n",
    "    po = data.groupby(d).size().div(len(data))\n",
    "    p0.append(po[0])\n",
    "    p1.append(po[1])\n",
    "    s0.append({\n",
    "        0:a[0][0],\n",
    "        1:a[0][1]\n",
    "    })\n",
    "    s1.append({\n",
    "        0:a[1][0],\n",
    "        1:a[1][1]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: 0.6867816091954023, 1: 0.49321266968325794},\n",
       " {0: 0.9741379310344828, 1: 0.8959276018099548},\n",
       " {0: 0.6752873563218391, 1: 0.3619909502262444},\n",
       " {0: 0.14655172413793105, 1: 0.6561085972850679},\n",
       " {0: 0.7758620689655172, 1: 0.6651583710407241},\n",
       " {0: 0.07758620689655171, 1: 0.16289592760180996}]"
      ]
     },
     "execution_count": 738,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_c = [s1,s0]\n",
    "#probs_c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictPior(x,probs_priori):\n",
    "    prob = 1\n",
    "    for i,v in enumerate(probs_priori):\n",
    "        prob *= v[x[i]]\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictP(x,classes,probs_c,piors):\n",
    "    probabilities = []\n",
    "    for i, c in enumerate(classes):\n",
    "        prior = priors[i]\n",
    "        prior_pred = predictPior(x,probs_c[i])\n",
    "        probabilities.append(prior*prior_pred)\n",
    "    print(probabilities,classes[np.argmax(probabilities)])\n",
    "    return classes[np.argmax(probabilities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProbs(X,clasess,probs_c,priors):\n",
    "    probabilities = []\n",
    "    for d in X:\n",
    "        prob = predictP(d,classes,probs_c,priors)\n",
    "        probabilities.append(prob)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('X_test.csv')\n",
    "Y_test = pd.read_csv('Y_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0005480811099228432, 0.0036313949092306604] 0\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.0016023411042392609, 0.002036801244565984] 0\n",
      "[0.006930307516293635, 0.00011437712144834662] 1\n",
      "[0.0034167567663925405, 0.0009043553219115799] 1\n",
      "[0.0008765491639946585, 0.002761022388705836] 0\n",
      "[0.004266862166385168, 0.00027646600993628926] 1\n",
      "[0.005941458208538918, 0.0001708689889510281] 1\n",
      "[0.0014237055294381675, 0.0011422662523591458] 1\n",
      "[0.0024537461455688644, 0.0014632468357684315] 1\n",
      "[0.0006542892842310314, 0.003352888202593236] 0\n",
      "[0.003924289842531316, 0.0011125359193212968] 1\n",
      "[0.0005480811099228432, 0.0036313949092306604] 0\n",
      "[0.011472847368379016, 0.000624006642554349] 1\n",
      "[0.004577417603093354, 0.0007447147474860405] 1\n",
      "[0.004416679425356996, 0.0019837884034636694] 1\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.0012205644388124503, 0.0017064416133398728] 0\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.0016023411042392609, 0.002036801244565984] 0\n",
      "[0.0008765491639946585, 0.002761022388705836] 0\n",
      "[0.004266862166385168, 0.00027646600993628926] 1\n",
      "[0.0005480811099228432, 0.0036313949092306604] 0\n",
      "[0.0008765491639946585, 0.002761022388705836] 0\n",
      "[0.0021036339504394304, 0.002185957333490466] 0\n",
      "[0.006930307516293635, 0.00011437712144834662] 1\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.0021036339504394304, 0.002185957333490466] 0\n",
      "[0.005464438078473625, 0.0006875994909824179] 1\n",
      "[0.0061500735676455545, 0.0012260744779104148] 1\n",
      "[0.0005480811099228432, 0.0036313949092306604] 0\n",
      "[0.003924289842531316, 0.0011125359193212968] 1\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.0008765491639946585, 0.002761022388705836] 0\n",
      "[0.0006542892842310314, 0.003352888202593236] 0\n",
      "[0.0010224350734138958, 0.0018481867014799316] 0\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.0008765491639946585, 0.002761022388705836] 0\n",
      "[0.0008765491639946585, 0.002761022388705836] 0\n",
      "[0.0008765491639946585, 0.002761022388705836] 0\n",
      "[0.0014237055294381675, 0.0011422662523591458] 1\n",
      "[0.0005480811099228432, 0.0036313949092306604] 0\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.0013737114383855142, 0.003042795315585522] 0\n",
      "[0.005464438078473625, 0.0006875994909824179] 1\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.006930307516293635, 0.00011437712144834662] 1\n",
      "[0.0008765491639946585, 0.002761022388705836] 0\n",
      "[0.0012205644388124503, 0.0017064416133398728] 0\n",
      "[0.0008765491639946585, 0.002761022388705836] 0\n",
      "[0.0014237055294381675, 0.0011422662523591458] 1\n",
      "[0.011472847368379016, 0.000624006642554349] 1\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.0010224350734138958, 0.0018481867014799316] 0\n",
      "[0.0012205644388124503, 0.0017064416133398728] 0\n",
      "[0.0008765491639946585, 0.002761022388705836] 0\n",
      "[0.0061500735676455545, 0.0012260744779104148] 1\n",
      "[0.0013737114383855142, 0.003042795315585522] 0\n",
      "[0.0007631840173208547, 0.002244372741391045] 0\n",
      "[0.0005480811099228432, 0.0036313949092306604] 0\n",
      "[0.0029292376259691106, 0.0013510243792706235] 1\n",
      "[0.006373895928972577, 0.0004602687179852018] 1\n",
      "[0.0010224350734138958, 0.0018481867014799316] 0\n",
      "[0.0029292376259691106, 0.0013510243792706235] 1\n",
      "[0.0005480811099228432, 0.0036313949092306604] 0\n",
      "[0.003924289842531316, 0.0011125359193212968] 1\n",
      "[0.0012205644388124503, 0.0017064416133398728] 0\n",
      "[0.011472847368379016, 0.000624006642554349] 1\n",
      "[0.003924289842531316, 0.0011125359193212968] 1\n",
      "[0.005464438078473625, 0.0006875994909824179] 1\n",
      "[0.0012205644388124503, 0.0017064416133398728] 0\n",
      "[0.0014237055294381675, 0.0011422662523591458] 1\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.005464438078473625, 0.0006875994909824179] 1\n",
      "[0.0013271140600166894, 0.00042405202010657245] 1\n",
      "[0.0010224350734138958, 0.0018481867014799316] 0\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.0010224350734138958, 0.0018481867014799316] 0\n",
      "[0.0007631840173208547, 0.002244372741391045] 0\n",
      "[0.0013737114383855142, 0.003042795315585522] 0\n",
      "[0.005464438078473625, 0.0006875994909824179] 1\n",
      "[0.000986531783654354, 0.004923242567987932] 0\n",
      "[0.0008765491639946585, 0.002761022388705836] 0\n",
      "[0.003924289842531316, 0.0011125359193212968] 1\n",
      "[0.004977004954808878, 0.00018506217300721318] 1\n",
      "[0.0021036339504394304, 0.002185957333490466] 0\n",
      "[0.003924289842531316, 0.0011125359193212968] 1\n",
      "[0.0015479884267963844, 0.0002838540199828012] 1\n",
      "[0.0008765491639946585, 0.002761022388705836] 0\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.002562633029904873, 0.001548620840794759] 1\n",
      "[0.006373895928972577, 0.0004602687179852018] 1\n",
      "[0.0061500735676455545, 0.0012260744779104148] 1\n",
      "[0.0005480811099228432, 0.0036313949092306604] 0\n",
      "[0.005464438078473625, 0.0006875994909824179] 1\n",
      "[0.0013737114383855142, 0.003042795315585522] 0\n",
      "[0.0008765491639946585, 0.002761022388705836] 0\n",
      "[0.0010224350734138958, 0.0018481867014799316] 0\n",
      "[0.0013737114383855142, 0.003042795315585522] 0\n",
      "[0.0012205644388124503, 0.0017064416133398728] 0\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.0006542892842310314, 0.003352888202593236] 0\n",
      "[0.0029292376259691106, 0.0013510243792706235] 1\n",
      "[0.0029292376259691106, 0.0013510243792706235] 1\n",
      "[0.003924289842531316, 0.0011125359193212968] 1\n",
      "[0.0029292376259691106, 0.0013510243792706235] 1\n",
      "[0.0012205644388124503, 0.0017064416133398728] 0\n",
      "[0.0034167567663925405, 0.0009043553219115799] 1\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.005464438078473625, 0.0006875994909824179] 1\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.0021036339504394304, 0.002185957333490466] 0\n",
      "[0.0013271140600166894, 0.00042405202010657245] 1\n",
      "[0.003924289842531316, 0.0011125359193212968] 1\n",
      "[0.0029292376259691106, 0.0013510243792706235] 1\n",
      "[0.006930307516293635, 0.00011437712144834662] 1\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.005464438078473625, 0.0006875994909824179] 1\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.0005480811099228432, 0.0036313949092306604] 0\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.0008765491639946585, 0.002761022388705836] 0\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.0010224350734138958, 0.0018481867014799316] 0\n",
      "[0.0005480811099228432, 0.0036313949092306604] 0\n",
      "[0.0006542892842310314, 0.003352888202593236] 0\n",
      "[0.0008765491639946585, 0.002761022388705836] 0\n",
      "[0.0008765491639946585, 0.002761022388705836] 0\n",
      "[0.0024537461455688644, 0.0014632468357684315] 1\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.0029292376259691106, 0.0013510243792706235] 1\n",
      "[0.0008765491639946585, 0.002761022388705836] 0\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.0007631840173208547, 0.002244372741391045] 0\n",
      "[0.0029292376259691106, 0.0013510243792706235] 1\n",
      "[0.0012205644388124503, 0.0017064416133398728] 0\n",
      "[0.0008765491639946585, 0.002761022388705836] 0\n",
      "[0.0008765491639946585, 0.002761022388705836] 0\n",
      "[0.006373895928972577, 0.0004602687179852018] 1\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.0024537461455688644, 0.0014632468357684315] 1\n",
      "[0.003924289842531316, 0.0011125359193212968] 1\n",
      "[0.0021036339504394304, 0.002185957333490466] 0\n",
      "[0.0008765491639946585, 0.002761022388705836] 0\n",
      "[0.005464438078473625, 0.0006875994909824179] 1\n",
      "[0.006930307516293635, 0.00011437712144834662] 1\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.0012205644388124503, 0.0017064416133398728] 0\n",
      "[0.0021036339504394304, 0.002185957333490466] 0\n",
      "[0.0021036339504394304, 0.002185957333490466] 0\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.004577417603093354, 0.0007447147474860405] 1\n",
      "[0.0008765491639946585, 0.002761022388705836] 0\n",
      "[0.0034167567663925405, 0.0009043553219115799] 1\n",
      "[0.0008765491639946585, 0.002761022388705836] 0\n",
      "[0.0024537461455688644, 0.0014632468357684315] 1\n",
      "[0.0071736431656386395, 0.000820715744416734] 1\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.0005480811099228432, 0.0036313949092306604] 0\n",
      "[0.006373895928972577, 0.0004602687179852018] 1\n",
      "[0.0029292376259691106, 0.0013510243792706235] 1\n",
      "[0.0016023411042392609, 0.002036801244565984] 0\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.006373895928972577, 0.0004602687179852018] 1\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.005151756613321055, 0.0013279180063259838] 1\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.006930307516293635, 0.00011437712144834662] 1\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n",
      "[0.00046987828488709527, 0.0054249728334208125] 0\n"
     ]
    }
   ],
   "source": [
    "pp = getProbs(np.array(X_test),classes,probs_c,priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7391304347826086"
      ]
     },
     "execution_count": 824,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "metrics.recall_score(Y_test,pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "fitx = gnb.fit(X_train, Y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = fitx.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7246376811594203"
      ]
     },
     "execution_count": 828,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.recall_score(Y_test,pred)\n",
    "\n",
    "#fitx.predict_proba(X_valid)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92793499 0.07206501]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(np.exp(fitx.predict_log_proba(X_valid)[1]))\n",
    "print(pred[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
